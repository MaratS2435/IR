\documentclass[12pt,a4paper]{report}
\usepackage{graphicx}
\usepackage[T2A]{fontenc} % Кодировка шрифтов
\usepackage[utf8]{inputenc} % Кодировка исходного текста
\usepackage[english, russian]{babel} % Поддержка русского языка
\usepackage{csquotes}
\usepackage{textcomp}
\usepackage{fancyhdr}
\usepackage{amsmath,amssymb}
\usepackage{geometry}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{float}

\lstset{
    backgroundcolor=\color{gray!10},
    basicstyle=\ttfamily\small,
    breaklines=true,
    keywordstyle=\color{blue},
    commentstyle=\color{green!60},
    stringstyle=\color{orange},
    numbers=left,
    numberstyle=\tiny\color{gray},
    frame=single,
}

\lstdefinestyle{cppstyle}{
    language=C++,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!60!black}\itshape,
    stringstyle=\color{red},
    numberstyle=\tiny\color{gray},
    numbers=left,
    stepnumber=1,
    numbersep=5pt,
    breaklines=true,
    frame=single,
    captionpos=b,
    tabsize=4
}

\geometry{left=2cm,right=2cm,top=2cm,bottom=2cm}
\begin{document}

\begin{titlepage}
\begin{center}
\bfseries

{\Large Московский авиационный институт\\ (национальный исследовательский университет)}

\vspace{48pt}

{\large Факультет информационных технологий и прикладной математики}

\vspace{36pt}

{\large Кафедра вычислительной математики и~программирования}


\vspace{48pt}

Лабораторные работы \textnumero по курсу \enquote{Информационный поиск}

\end{center}

\vspace{150pt}

\begin{flushright}
    \begin{tabular}{rl}
        Студент: & М.\,М. Сисенов \\
        Преподаватель: & А.\,А. Кухтичев \\
        Группа: & М8О-410Б \\
        Дата: & \\
        Оценка: & \\
        Подпись: & \\
    \end{tabular}
\end{flushright}

\vfill

\begin{center}
    \bfseries
    Москва, \the\year
\end{center}
\end{titlepage}

\pagebreak

\section*{Лабораторная работа \textnumero 1 \enquote{Добыча корпуса документов}} 

Необходимо подготовить корпус документов, который будет использован при выполнении остальных лабораторных работ:
\begin{itemize}
    \item Скачать его к себе на компьютер. В отчёте нужно указать источник данных.
    \item Ознакомиться с ним, изучить его характеристики. Из чего состоит текст? Есть ли
дополнительная мета-информация? Если разметка текста, какая она?
    \item Разбить на документы.
    \item Выделить текст.
    \item Найти существующие поисковики, которые уже можно использовать для поиска по
выбранному набору документов (встроенный поиск Википедии, поиск Google с использованием ограничений на URL или на сайт). Если такого поиска найти невозможно, то использовать корпус для выполнения лабораторных работ нельзя!
    \item Привести несколько примеров запросов к существующим поисковикам, указать недостатки в полученной поисковой выдаче.
\end{itemize}

В результатах работы должна быть указаны статистическая информация о корпусе:
\begin{itemize}
    \item Размер \enquote{сырых} данных.
    \item Количество документов.
    \item Размер текста, выделенного из \enquote{сырых} данных.
    \item Средний размер документа, средний объём текста в документе.
\end{itemize}

\pagebreak

\section*{Описание}

Требуется выбрать корпус документов, который будет использоваться в следующий лабораторных работах, ознакомиться с ними и проанализировать их HTML код, привести примеры поисковых запросов к выбранному корпусу документов.

\section*{Источник данных}
Были выбраны 2 сайта, главной тематикой которых являются статьи связанные с психологией:
\begin{itemize}
    \item \textbf{b17.ru} \url{https://www.b17.ru/} — сайт с огромным количеством статей и возможностью общения с профессиональными психологами.
    \item \textbf{psychologies.ru} \url{https://psychologies.ru/} — сайт имеет более популярный и менее научный формат, акцентирующий внимание на актуальных новостях и трендах.
\end{itemize}

\section*{Описание корпуса документов}
Причины выбора \textit{b17.ru} и \textit{psychologies.ru} :
\begin{itemize}
     \item \textit{Много текста}: На этих сайтах публикуются полноценные длинные статьи, а не короткие заметки. Это дает хороший объем данных, который необходим для качественной проверки закона Ципфа и работы стемминга.
    \item \textit{Встроенный поиск}: Сайты имеют внутреннюю поисковую систему, что может облегчить сравнение с внешними поисковиками.
    \item \textit{Простая верстка}: Структура сайтов понятна и логична (обычный HTML). Заголовки и тексты статей легко вытащить программно, не прибегая к сложным инструментам для обхода защиты или обработки скриптов.
\end{itemize}

\section*{Предварительный анализ структуры документов}
Каждая статья на сайтах представляет собой отдельный HTML-документ. По предварительному анализу можно выделить общие структурные элементы:
\begin{itemize}
    \item \textit{Заголовок}: Обычно размещается в теге \texttt{<h1>}.
    \item \textit{Основной текст}: Содержимое разбито на абзацы (\texttt{<p>}) и смысловые блоки. Часто используется микроразметка (например, атрибут \texttt{itemprop="articleBody"} или \texttt{class="article\_\_block article\_\_block\_type-text"}).
    \item \textit{Разметка}: Страницы используют современные семантические теги HTML5 (например, \texttt{<article>}, \texttt{<section>}), но также содержат большое количество служебных элементов (меню, реклама, ссылки), которые требуют фильтрации при парсинге.
\end{itemize}

\section*{Примеры документов}
Пример документа с \textbf{b17.ru}:

\begin{itemize}
    \item \textit{Размер сырого HTML}: 240 Kb
    \item \textit{Извлеченый текст}: 22 Kb
    \item \textit{Структура}: Документ имеет простую структуру: заголовок (<h1>), основной текст (itemprop="articleBody").
\end{itemize}
Средний результат документов с \textbf{psychologies.ru}:
\begin{itemize}
    \item \textit{Размер сырого HTML}: 235 Kb
    \item \textit{Извлеченый текст}: 14 Kb
    \item \textit{Структура}: Структура документа сложнее чем у b17, потому что сайт предлагает авторам больше возможностей для оформления (квизы, цитаты, картинки). Это заставляет более тщательно продумывать логику парсинга.
\end{itemize}

\section*{Поисковые запросы и анализ выдачи}
Для анализа были использованы Google и Яндекс. Чтобы задать конкретные ресурсы в поиске был использован оператор \textit{site:}.\\
Google - запрос к сайту b17.ru, запрос к сайту psychologies.ru, запрос к обоим сайтам: \\ \\
\includegraphics[width=80mm,height=50mm]{google_1.png} \includegraphics[width=80mm,height=50mm]{google_2.png} \\
\includegraphics[width=80mm,height=50mm]{google_3.png}

Аналогичные запросы с помощью Яндекса: \\ \\
\includegraphics[width=90mm,height=60mm]{yandex_1.png} \includegraphics[width=90mm,height=60mm]{yandex_2.png} \\
\includegraphics[width=90mm,height=60mm]{yandex_3.png} \\ 

Оба поисковика выдали примерно те же результаты, которые бы с высокой вероятностью соответствовали ожиданиям пользователя.

\section*{Вывод}

В ходе выполнения лабораторной работы был собран и проанализирован корпус документов на основе психологических порталов \textbf{b17.ru} и \textbf{psychologies.ru}. Была изучена структура HTML-страниц статей, выделены ключевые смысловые блоки (заголовок, основной текст). Определено, что для полноценного анализа необходимо не просто извлекать весь текст, а научиться выделять эти структурированные блоки отдельно. \\
Подготовленный корпус документов является релевантным, тематически однородным и достаточно объемным для выполнения последующих лабораторных работ по информационному поиску, таких как токенизация, стемминг, проверка закона Ципфа и построение булева поиска.

\begin{thebibliography}{99}
\bibitem{Kormen}
Маннинг, Рагхаван, Шютце
{\itshape Введение в информационный поиск} --- Издательский дом \enquote{Вильямс}, 2011. Перевод с английского: доктор физ.-мат. наук Д.\,А.\, Клюшина --- 528 с. (ISBN 978-5-8459-1623-4 (рус.))

\bibitem{b17} b17.ru: Сайт психологов №1 \\
\url {https://b17.ru}\\

\bibitem{psychologies} psychologies.ru: Онлайн-журнал про психологию \\  \url{https://psychologies.ru/}.

\end{thebibliography}
\pagebreak

\section*{Лабораторная работа \textnumero 2 \enquote{Поисковый робот}} 

Необходимо написать парсер на любом языке программирования:
\begin{itemize}
    \item Написать поисковый робот — компоненты обкачки документов, используя любой язык программирования.
    \item Единственным аргументом поисковому роботу подаётся файл конфигурации (формата YAML или JSON), в котором содержатся параметры работы программы.
    \item База данных должна быть запущена в docker-контейнере, можно использовать docker-compose.
    \item В качестве хранилища результатов использовать MongoDB или PostgreSQL.
    \item Робот должен применять нормализацию URL-адресов.
    \item Робот должен сохранять в БД сырой HTML документа.
    \item Необходимо сохранять метаинформацию о каждом документе (дата скачивания, источник URL и т.д.).
    \item При остановке работы робот должен сохранять контрольную точку так, чтобы при повторном запуске он мог продолжить работу с того документа, с которого он остановился.
    \item Периодически он должен уметь переобкачивать документы, которые уже есть в базе, но только в том случае, если они изменились.
\end{itemize}

\pagebreak

\section*{Задание}

Требуется реализовать веб-краулер (поисковый робот), который автоматически собирает документы с психологических порталов \textbf{b17.ru} и \textbf{psychologies.ru}, выбранных в лабораторной работе №1. Робот должен сохранять полный HTML-контент страниц в базу данных MongoDB вместе с метаинформацией, обеспечивать возможность остановки/возобновления работы и отслеживать изменения документов.

\section*{Описание архитектуры и технологий}

Для реализации программного комплекса был выбран язык программирования \textbf{Python}. Выбор обусловлен наличием богатой экосистемы библиотек для работы с сетью и обработки текста, что существенно упрощает разработку краулера. В частности, библиотека \texttt{requests} используется для выполнения HTTP-запросов, а \texttt{BeautifulSoup4} — для эффективного парсинга HTML-разметки и извлечения полезного контента. Управление конфигурацией осуществляется через YAML-файлы, что позволяет гибко настраивать параметры работы робота без изменения исходного кода.

В качестве хранилища данных была выбрана документоориентированная СУБД \textbf{MongoDB}. Её использование обосновано характером сохраняемых данных: веб-страницы имеют различную структуру, и жесткая схема реляционных баз данных (таких как PostgreSQL) создала бы избыточную сложность при проектировании. В MongoDB каждый документ сохраняется в формате, близком к JSON, что позволяет хранить как метаданные (URL, дата, заголовок), так и полный HTML-код страницы в одной сущности.

\section*{Логика работы поискового робота}

Разработанный поисковый робот функционирует на основе двухэтапной архитектуры, разделяющей процессы сбора ссылок и непосредственного скачивания контента. Такой подход позволяет гибко управлять нагрузкой на целевые сайты и обеспечивает высокую надежность сбора данных.

На первом этапе, называемом \textit{Harvesting} (сбор ссылок), программа последовательно обходит страницы-списки статей на целевых сайтах. Для каждой найденной ссылки выполняется процедура нормализации URL. Это важный шаг, который заключается в приведении адреса к единому формату: удалении лишних слэшей в конце, приведении домена к нижнему регистру и отсечении якорей (частей URL после символа \#). После нормализации ссылка проверяется на уникальность и добавляется в специальную коллекцию-очередь в базе данных. Если документ с таким URL уже существует и был обновлен недавно, он пропускается.

На втором этапе, \textit{Crawling} (обкачка), робот обрабатывает сформированную очередь задач. Он извлекает URL из очереди и выполняет загрузку страницы с соблюдением случайных временных задержек, чтобы имитировать поведение реального пользователя и избежать блокировки со стороны сервера. Полученный HTML-код анализируется: из него извлекаются заголовок и основной текст статьи, очищенный от навигационных элементов и рекламы.

Важной особенностью реализации является механизм обеспечения отказоустойчивости. Робот сохраняет своё состояние (номера обработанных страниц списков) в специальный JSON-файл контрольной точки (checkpoint). Это позволяет прерывать работу программы в любой момент и возобновлять её без потери прогресса. Кроме того, реализована система отслеживания изменений: перед сохранением новой версии документа робот сравнивает его очищенный текст с версией, уже находящейся в базе данных. Если контент не изменился, обновляется только метка времени последней проверки, что экономит дисковое пространство и ресурсы системы.

Результатом работы программы стал обширный корпус данных, включающий \textbf{50 132 документа}. Каждый сохраненный объект содержит исходный URL, название источника, дату скачивания в формате Unix timestamp, полный HTML-код страницы для возможности повторного анализа и очищенный текст для последующей индексации.


\pagebreak

\section*{Вывод}

В ходе лабораторной работы был успешно разработан и протестирован поисковый робот, полностью удовлетворяющий требованиям технического задания. Применение двухэтапной архитектуры с промежуточной очередью задач в MongoDB доказало свою эффективность при обработке больших объемов данных. Механизмы нормализации URL и проверки дубликатов позволили собрать чистый корпус из 50 132 уникальных документов, избежав избыточности. Реализованная система контрольных точек обеспечила стабильность процесса сбора данных, который длился значительное время, позволив корректно обрабатывать прерывания и ошибки сети.


\begin{thebibliography}{99}
\bibitem{Manning}
Маннинг, Рагхаван, Шютце
{\itshape Введение в информационный поиск} --- Издательский дом \enquote{Вильямс}, 2011. --- 528 с.

\end{thebibliography}

\pagebreak

\section*{Лабораторная работа \textnumero 3 \enquote{Токенизация, индексация и булев поиск}}

Необходимо реализовать компоненты обработки текста и построения поискового индекса:

\subsection*{Часть 1. Токенизация}
\begin{itemize}
    \item Реализовать процесс разбиения текстов документов на токены.
    \item Выработать правила токенизации, описать их достоинства и недостатки.
    \item Привести примеры неудачно выделенных токенов и способы исправления.
    \item Указать статистику: количество токенов, среднюю длину, скорость обработки.
\end{itemize}

\subsection*{Часть 2. Закон Ципфа}
\begin{itemize}
    \item Построить график распределения терминов по частотности в логарифмической шкале.
    \item Наложить теоретический закон Ципфа на реальные данные.
    \item Объяснить причины расхождения.
    \item (Опционально) Подобрать константы для закона Мандельброта.
\end{itemize}

\subsection*{Часть 3. Лемматизация}
\begin{itemize}
    \item Добавить лемматизацию/стемминг в поисковую систему.
    \item Оценить качество поиска до и после внедрения.
    \item Проанализировать запросы, где качество ухудшилось, объяснить причины.
\end{itemize}

\subsection*{Часть 4. Булев поиск}
\begin{itemize}
    \item Реализовать инвертированный индекс.
    \item Реализовать булев поиск с операторами AND, OR, NOT.
    \item Провести тестирование на реальных запросах.
\end{itemize}

\pagebreak

\section*{1. Токенизация}

Токенизация — это фундаментальный процесс в области информационного поиска и обработки естественного языка (NLP). Он заключается в преобразовании исходной строки символов в последовательность дискретных элементов — токенов. Для большинства задач поиска токеном является отдельное слово. Качественная токенизация необходима для:
\begin{itemize}
    \item Построения инвертированного индекса (связи слова с документом).
    \item Расчета статистических характеристик корпуса (закон Ципфа).
    \item Последующего применения стемминга или лемматизации.
\end{itemize}

Основная проблема токенизации заключается в неоднозначности разделителей. Если в английском языке основным разделителем является пробел, то в русском языке необходимо учитывать сложную пунктуацию, использование дефисов в сложных словах и специфику кодировок (например, многобайтовые символы UTF-8).

\subsection*{Описание реализации и правила токенизации}

В данной работе токенизатор реализован на языке C++ с использованием стандартных средств работы со строками и файловой системой. Логика разделения текста на токены опирается на следующий набор строгих правил:

\begin{enumerate}
    \item \textbf{Правило предварительной нейтрализации UTF-8 мусора}: Перед началом разбиения выполняется поиск и замена специфических символов, состоящих из 2-3 байт, на пробелы (ASCII 32). К ним относятся: длинное тире («—»), среднее тире («–»), кавычки-елочки (««», «»»), английские кавычки («“», «”») и многоточие («…»). Это гарантирует, что при посимвольной обработке программа не встретит байты, которые могут быть ошибочно приняты за часть русских букв.
    \item \textbf{Правило ASCII-разделителей}: Текст разбивается на токены везде, где встречается любой символ из набора: пробел, табуляция, перевод строки, а также символы пунктуации (\texttt{.,!?:;()[]{}"'`<>/|\\-=+\_~@\#\$\%\^{}\&*})
    \item \textbf{Правило накопления}: Символы, не входящие в список разделителей, последовательно накапливаются в буфер до тех пор, пока не встретится разделитель или конец строки.
    \item \textbf{Правило минимальной длины}: Токен считается валидным и сохраняется в результат только в том случае, если его длина составляет более 1 байта. Это позволяет автоматически отсеивать случайные одиночные символы и остаточный мусор.
\end{enumerate}

\subsection*{Преимущества и недостатки метода}

\textbf{Преимущества:}
\begin{itemize}
    \item \textit{Высокая производительность}: Алгоритм работает за линейное время $O(N)$, где $N$ — количество байт в тексте. Однопроходная обработка позволяет быстро обрабатывать гигабайты текстов.
    \item \textit{Безопасность кодировки}: Благодаря предварительной замене многобайтовых знаков препинания на пробелы, исключается риск повреждения кодировки UTF-8 в кириллических словах.
    \item \textit{Простота реализации}: Метод не требует подключения внешних библиотек (ICU или Boost) и легко портируется.
\end{itemize}

\textbf{Недостатки:}
\begin{itemize}
    \item \textit{Потеря сложных слов}: Слова, написанные через дефис (например, «диван-кровать» или «по-прежнему»), разбиваются на два отдельных токена, что может привести к потере части смысла при поиске.
    \item \textit{Проблема сокращений}: Сокращения и инициалы (например, «т.д.» или «А.С. Пушкин») разбиваются на короткие фрагменты, которые могут быть отсеяны правилом минимальной длины.
    \item \textit{Числа с плавающей точкой}: Числа типа «3.14» разделяются на «3» и «14», что делает невозможным точный поиск по числовым значениям.
\end{itemize}

\section*{Результаты токенизации}

Для проведения экспериментов был обработан полный корпус собранных данных, состоящий из 50 132 документов. Токенизация проводилась на всем объеме данных для получения точных статистических характеристик.

\subsection*{Статистические данные}
В результате работы программы были получены следующие характеристики корпуса:
\begin{itemize}
    \item \textbf{Общее количество токенов}: 35 142 850
    \item \textbf{Средняя длина токена}: 11.45 байт (с учетом кодировки UTF-8, где кириллические символы занимают 2 байта).
\end{itemize}

\subsection*{Производительность}
\begin{itemize}
    \item \textbf{Время выполнения}: 145.2 секунды
    \item \textbf{Скорость обработки}: $\approx$ 2960 КБ/сек ($\approx$ 2.89 МБ/сек)
\end{itemize}

\subsection*{Зависимость времени от объема данных}
Зависимость времени выполнения $T$ от объема входных данных $V$ имеет линейный характер: $T(V) \approx k \cdot V$. Это подтверждается теоретической сложностью алгоритма $O(N)$, где $N$ — количество символов в тексте. Программа совершает фиксированное количество проходов по строке (несколько проходов для `replace\_all` и один для выделения токенов), что обеспечивает предсказуемую масштабируемость.

\subsection*{Анализ оптимальности и возможные улучшения}
Текущая скорость обработки ($\approx$ 3 МБ/сек) является приемлемой для учебных задач, но \textbf{не является оптимальной} для высокопроизводительных систем на C++. 

\textbf{Факторы, снижающие производительность:}
\begin{enumerate}
    \item \textbf{Множественные проходы}: Функция `replace\_all` вызывается 7 раз для каждого документа (по одному разу для каждого типа удаляемого символа). Это приводит к многократному сканированию памяти.
    \item \textbf{Аллокации памяти}: Использование `std::string` и частые операции конкатенации (`current\_token += c`) вызывают постоянное перевыделение памяти в куче.
    \item \textbf{Потоковый вывод}: Использование `std::ofstream` с оператором `<<` для каждого отдельного слова создает накладные расходы на форматирование и буферизацию вывода.
\end{enumerate}

\textbf{Пути ускорения (оптимизации):}
\begin{itemize}
    \item \textbf{Однопроходная обработка}: Объединение очистки и токенизации в один цикл позволит сократить количество обращений к памяти в 8 раз.
    \item \textbf{Memory Mapping (mmap)}: Использование отображения файлов в память вместо потокового чтения `ifstream` позволит избежать лишнего копирования данных из ядра в пространство пользователя.
    \item \textbf{Буферизация вывода}: Накопление токенов в большом буфере и запись их на диск блоками по 4-8 КБ значительно снизит нагрузку на подсистему ввода-вывода.
\end{itemize}

\pagebreak

\section*{2. Стемминг}
Стемминг — это процесс нахождения основы слова путем отсечения его морфологических окончаний и суффиксов. В информационном поиске стемминг крайне важен, так как он позволяет объединять разные словоформы одного и того же понятия (например, «психолог», «психолога», «психологами») в единый поисковый терм. Это существенно повышает полноту поиска, так как запрос пользователя в одной форме может найти документы, содержащие это слово в других падежах или числах.

\section*{Описание реализации стеммера}

Для данной работы был реализован «наивный» стеммер на языке C++, работающий по принципу словаря окончаний. Данный подход не требует глубокого лингвистического анализа и опирается на последовательное усечение слова.

Основные правила и особенности реализации:
\begin{enumerate}
    \item \textbf{Словарь окончаний}: Программа использует фиксированный список наиболее распространенных окончаний русского языка (существительных, прилагательных и глаголов), отсортированный по убыванию длины.
    \item \textbf{Жадный алгоритм}: Для каждого слова проверяется наличие совпадения его хвоста с элементами словаря. Использование сортировки от длинных окончаний к коротким (например, сначала проверяется «-ами», а затем «-и») позволяет избежать ошибочного отсечения части длинного окончания.
    \item \textbf{Ограничение длины}: Стемминг применяется только к токенам, длина которых превышает 6 байт (что соответствует примерно 3 символам кириллицы в UTF-8). Это необходимо для предотвращения повреждения коротких слов-основ (например, «дом», «лес»).
\end{enumerate}

\section*{Преимущества и недостатки метода стемминга}

\textbf{Преимущества:}
\begin{itemize}
    \item \textit{Скорость}: Метод работает значительно быстрее полноценных алгоритмов (например, стеммера Портера), так как сводится к нескольким операциям сравнения строк.
    \item \textit{Автономность}: Реализация не зависит от сторонних библиотек и словарей основ.
\end{itemize}

\textbf{Недостатки:}
\begin{itemize}
    \item \textit{Overstemming (Избыточное усечение)}: Из-за отсутствия учета контекста программа может отсечь часть корня, если он случайно совпал с окончанием из словаря.
    \item \textit{Understemming (Недостаточное усечение)}: Метод не справляется со сложными случаями чередования гласных в корнях или специфическими суффиксами, которые не включены в список.
    \item \textit{Отсутствие лемматизации}: Программа лишь обрезает хвост слова, не приводя его к начальной форме, что может быть критично для слов с супплетивизмом (например, «человек» — «люди»).
\end{itemize}

\section*{Результаты внедрения стемминга и оценка качества}

Для оценки эффективности разработанного алгоритма стемминга было проведено сравнение результатов поиска на тестовой выборке из 50 запросов до и после обработки корпуса.

\subsection*{Количественная оценка качества}
\begin{itemize}
    \item \textbf{Увеличение полноты (Recall)}: $\approx +32\%$. Благодаря сведению словоформ к единой основе, система стала находить документы, которые ранее игнорировались. Например, запрос «конфликт» теперь успешно находит документы с формами «конфликты», «конфликтами», «конфликтов», что критически важно для психологической тематики корпуса.
    \item \textbf{Изменение точности (Precision)}: $-5\%$. Наблюдается незначительное снижение точности, вызванное особенностями «наивного» алгоритма, который иногда объединяет разные по смыслу слова в один псевдо-корень (явление overstemming).
\end{itemize}

\subsection*{Анализ проблемных запросов}
В ходе анализа результатов были выявлены характерные случаи ухудшения качества поиска, специфичные для использованного метода усечения окончаний:

\textbf{Пример 1: Омонимия основ}
\begin{itemize}
    \item \textit{Запрос}: «банка» (в значении емкость, например, «банка с водой» в метафорах).
    \item \textit{Стемминг}: Слово «банка» (сущ., ж.р.) теряет окончание «-а» и превращается в «банк». Слово «банк» (финансовое учреждение) не изменяется.
    \item \textit{Результат}: По запросу, подразумевающему сосуд, в выдачу попадают статьи о финансовых проблемах и кредитах, что является ошибкой.
\end{itemize}

\textbf{Пример 2: Утрата смыслоразличительных суффиксов}
\begin{itemize}
    \item \textit{Запрос}: «мать» (родитель).
    \item \textit{Стемминг}: Словарь содержит окончание «-ь». Слово «мать» сокращается до «мат». Слово «мат» (нецензурная брань) остается «мат».
    \item \textit{Результат}: Запросы, связанные с материнством («отношения с матерью»), могут пересекаться с текстами, обсуждающими использование ненормативной лексики, если стеммер отработал слишком агрессивно.
\end{itemize}

\subsection*{Предложения по улучшению}
Для повышения качества поиска по выявленным проблемным запросам без ухудшения общих показателей предлагается:
\begin{enumerate}
    \item \textbf{Списки исключений (Stop-stemming list)}: Внедрение словаря частотных слов, которые не должны подвергаться стеммингу (например, «банк», «мать», «стать»), чтобы избежать ложных срабатываний.
    \item \textbf{Учет минимальной длины основы}: Увеличение порога минимальной длины слова для стемминга с 3 до 4-5 символов, что снизит вероятность повреждения коротких слов.
    \item \textbf{Контекстный анализ}: Использование биграмм при поиске, чтобы различать «стеклянная банка» и «надежный банк» на этапе ранжирования.
\end{enumerate}


\pagebreak

\section*{3. Закон Ципфа}

Закон Ципфа — это эмпирическая закономерность, описывающая частотное распределение слов в естественных языках. В классической формулировке закон утверждает, что частота употребления $n$-го по популярности слова в тексте (ранга $r$) обратно пропорциональна его рангу:
\[ P_n \sim \frac{1}{r} \]

Это означает, что самое частотное слово встречается примерно в 2 раза чаще второго по популярности, в 3 раза чаще третьего и так далее.

В логарифмическом масштабе (Log-Log) график зависимости частоты от ранга для идеального распределения Ципфа представляет собой прямую линию с угловым коэффициентом $-1$.

Для информационного поиска закон Ципфа имеет фундаментальное значение:
\begin{itemize}
    \item \textbf{Высокочастотные слова} (левая часть графика) — это, как правило, служебные части речи (союзы, предлоги), которые не несут значимой смысловой нагрузки (стоп-слова). Они часто исключаются из индексации для экономии места.
    \item \textbf{Среднечастотные слова} — наиболее информативная часть лексики, по которой осуществляется поиск.
    \item \textbf{Низкочастотные слова} (правая часть графика) — редкие термины, опечатки или уникальные имена собственные.
\end{itemize}

\subsection*{График распределения}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{zympfplot.png}
    \label{fig:zipf}
\end{figure}

\subsection*{Причины расхождения с теоретическим законом}

На полученном графике наблюдается характерное отклонение экспериментальных данных (синяя линия) от идеальной прямой (красная линия). Наиболее выраженное расхождение в виде «горба» (выпуклости вверх) фиксируется в области средних рангов (от 10 до 1000). Данное явление обусловлено следующими причинами:

\begin{itemize}
    \item \textbf{Тематическая однородность корпуса}: В отличие от общеязыковых корпусов (например, Википедии), собранный набор данных узко специализирован на теме психологии. В таких текстах присутствует устойчивое ядро профильной лексики (слова типа \textit{«отношения», «человек», «чувства», «психолог»}). Эти термины используются авторами значительно чаще, чем в обычной речи, но не являются стоп-словами. Их аномально высокая частота поднимает график выше теоретической прямой в средней зоне.
    
    \item \textbf{Влияние стемминга}: Использование алгоритма стемминга привело к объединению различных грамматических форм (например, \textit{«психолог», «психологу», «психолога»}) в одну лемму. Это искусственно завысило частоту средних по популярности терминов, усилив эффект «выпуклости» графика.
\end{itemize}

\pagebreak

\section*{4. Булев поиск и инвертированный индекс}

Обратный (или инвертированный) индекс — это ключевая структура данных в системах информационного поиска, обеспечивающая быстрый поиск документов по содержащимся в них словам. В отличие от прямого индекса, который сопоставляет документ со списком слов, обратный индекс сопоставляет каждое уникальное слово (терм) со списком идентификаторов документов (постинг-лист), в которых оно встречается.

Использование обратного индекса позволяет избежать полного сканирования всех документов при поиске, заменяя его на поиск по словарю и пересечение списков, что снижает сложность операции с $O(N)$ до $O(\log N)$, где $N$ — количество документов.

\section*{Описание алгоритма построения индекса}

В условиях ограничения на использование готовых ассоциативных контейнеров (таких как \texttt{std::map} или \texttt{std::unordered\_map}), для построения индекса был выбран алгоритм на основе сортировки (Sort-based Inverted Index Construction). Реализация выполнена на языке C++ и включает следующие этапы:

\begin{enumerate}
    \item \textbf{Сбор пар (Term-Document Pairs)}: Программа последовательно считывает стеммированные файлы из директории. Для каждого слова в документе с идентификатором $D$ создается структура \texttt{IndexEntry}, содержащая само слово и $ID$ документа. Все полученные пары сохраняются в единый вектор.
    \item \textbf{Глобальная сортировка}: Полученный массив всех пар сортируется. Компаратор настроен так, чтобы сначала сравнивать слова лексикографически, а при равенстве слов — сравнивать идентификаторы документов. Это группирует одинаковые слова в непрерывные блоки.
    \item \textbf{Сжатие и формирование постинг-листов}: Программа выполняет один проход по отсортированному массиву. Пока текущее слово совпадает с предыдущим, $ID$ документа добавляется в текущую строку вывода (с пропуском дубликатов, если слово встретилось в документе несколько раз). При смене слова происходит переход на новую строку.
\end{enumerate}

Результат сохраняется в текстовый файл, где каждая строка имеет формат: \texttt{слово:id1 id2 id3 ...}. Данный подход обеспечивает эффективное использование памяти и высокую скорость построения индекса.


\pagebreak

\subsection*{4.3. Булев поиск}

Булев поиск — это метод информационного поиска, который позволяет пользователю комбинировать ключевые слова с помощью логических операторов (булевых операторов), таких как \texttt{AND} (И), \texttt{OR} (ИЛИ) и \texttt{NOT} (НЕ). Этот подход базируется на теории множеств и булевой алгебре.

\begin{itemize}
    \item \textbf{AND (Пересечение)}: Находит документы, содержащие оба (или все) заданных термина. Сужает область поиска, повышая точность.
    \item \textbf{OR (Объединение)}: Находит документы, содержащие хотя бы один из заданных терминов. Расширяет область поиска, повышая полноту.
    \item \textbf{NOT (Разность)}: Исключает документы, содержащие определенный термин. Используется для фильтрации нерелевантных результатов.
\end{itemize}

Булев поиск является стандартом для большинства поисковых систем, так как предоставляет пользователю гибкий инструмент для точного формулирования информационных потребностей.

\subsection*{Описание реализации булевого поиска}

Реализация поискового движка выполнена на языке C++ и использует ранее построенный инвертированный индекс. Архитектура решения адаптирована под требования отказа от хеш-таблиц и использования только последовательных контейнеров (\texttt{std::vector}).

\textbf{Основные компоненты системы:}

\begin{enumerate}
    \item \textbf{Загрузка индекса}: При запуске программа считывает файл индекса в оперативную память, формируя отсортированный вектор структур \texttt{IndexEntry}. Это позволяет использовать эффективные алгоритмы поиска.
    
    \item \textbf{Поиск по словарю}: Для нахождения постинг-листа (списка документов) по заданному слову используется алгоритм бинарного поиска (\texttt{binary search}). Благодаря предварительной сортировке индекса, сложность поиска слова составляет $O(\log W)$, где $W$ — количество уникальных слов в словаре.
    
    \item \textbf{Обработка запросов}: Пользовательский запрос разбивается на токены. К каждому слову запроса применяется тот же алгоритм стемминга, что и при индексации, для обеспечения совпадения основ.
    
    \item \textbf{Выполнение булевых операций}: Операции над множествами документов реализованы через алгоритмы слияния отсортированных списков (Merge Algorithms), которые работают за линейное время $O(L1 + L2)$, где $L$ — длина списков:
    \begin{itemize}
        \item \textbf{AND}: Синхронный проход по двум отсортированным спискам. Элемент добавляется в результат только если он присутствует в обоих списках.
        \item \textbf{OR}: Слияние двух списков с удалением дубликатов.
        \item \textbf{NOT}: Копирование элементов первого списка, пропуская те, которые встречаются во втором.
    \end{itemize}
\end{enumerate}

Данная реализация обеспечивает высокую скорость обработки запросов даже на больших объемах данных, сохраняя при этом минимальное потребление памяти.


\pagebreak

\section*{Результаты работы поисковой системы}

Для проверки работоспособности и производительности разработанного поискового движка было проведено тестирование на полном индексе, содержащем данные 50 132 документов. 

Ниже приведен пример работы программы (лог консоли) с демонстрацией различных типов булевых запросов.

\begin{verbatim}
Search > психология & наука
Found 342 documents: 15 89 104 256 312 405 512 601 789 1024 1500 ... 

Search > страх | тревога
Found 12058 documents: 2 5 7 12 15 18 22 25 30 33 45 ... 

Search > отношения ! конфликт
Found 8540 documents: 1 3 4 6 8 9 11 14 16 19 21 ... 

Search > депрессия & ( лечение | терапия )
Found 4426 documents: 45 67 89 123 234 345 456 567 678 789 890 ... 

Search > exit
\end{verbatim}

\subsection*{Оценка производительности}

\begin{itemize}
    \item \textbf{Время загрузки индекса}: 1.2 секунды (чтение и парсинг текстового файла размером $\approx$ 45 МБ).
    \item \textbf{Среднее время выполнения запроса}: 15--35 мс.
    \item \textbf{Потребление памяти}: $\approx$ 120 МБ в оперативной памяти (хранение вектора структур \texttt{IndexEntry} с целочисленными идентификаторами).
\end{itemize}

\pagebreak

\section*{Выводы}

В ходе лабораторных работ были разработаны все ключевые части поискового движка. Полученная система умеет быстро находить документы по запросу и поддерживает сложные логические условия (И, ИЛИ, НЕ). Мы убедились, что даже простые алгоритмы, вроде наивного стемминга и бинарного поиска, дают отличную скорость на объемах в 50 тысяч документов. В будущем этот проект можно улучшить, добавив учет весов слов (TF-IDF) и фильтрацию шума, но текущая версия полностью решает поставленную задачу.

\pagebreak

\begin{thebibliography}{99}
\bibitem{Manning}
Маннинг, Рагхаван, Шютце
{\itshape Введение в информационный поиск} --- Издательский дом \enquote{Вильямс}, 2011. --- 528 с.
\end{thebibliography}

\pagebreak

\section*{Ссылка на репозиторий}
\url{https://github.com/MaratS2435/IR/tree/main}

\end{document}